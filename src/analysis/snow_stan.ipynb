{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCGOsNC1F6EV"
   },
   "source": [
    "# Description\n",
    "This will focus on using classic regression models and bayesian models. \n",
    "\n",
    "#### NOTE: As described in EDA notebook, \"Pseudo_ts\" is concatenation of data from locally adjacent ski resorts (e.g., all resorts in Colorado) into a single timeseries.\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "5hkRrNSaGDae",
    "outputId": "309147c1-a228-4718-e332-d6cd07c4169a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vapeplot in /opt/conda/lib/python3.8/site-packages (0.0.8)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from vapeplot) (3.3.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from vapeplot) (1.19.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->vapeplot) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->vapeplot) (1.2.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/conda/lib/python3.8/site-packages (from matplotlib->vapeplot) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->vapeplot) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->vapeplot) (7.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from matplotlib->vapeplot) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->vapeplot) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install vapeplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "TNlQEkolNSWR",
    "outputId": "17551999-4d6a-4337-922b-4bf5c6488a2a"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    base_path = r'/content/gdrive/My Drive/data_sci/colab/ski/'\n",
    "    os.chdir(base_path)\n",
    "    try:\n",
    "        ! git clone https://github.com/chrisoyer/ski-snow-modeling/\n",
    "    except:  # if dir not empty e.g. already cloned\n",
    "        pass\n",
    "    mod_path = os.path.join(base_path, \n",
    "                            r\"ski-snow-modeling/src/analysis/project_utils/project_utils.py\")\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(name=\"utils.name\", location=mod_path)\n",
    "    utils = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(utils)\n",
    "    \n",
    "    os.chdir('./ski-snow-modeling/src/analysis/')\n",
    "    # Change the working directory to the repo root.\n",
    "    # Add the repo root to the Python path.\n",
    "    import sys\n",
    "    sys.path.append(os.getcwd())\n",
    "else:\n",
    "    # local running\n",
    "    import project_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "h7TktoEhF6EX"
   },
   "outputs": [],
   "source": [
    "# data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import pickle\n",
    "import calendar\n",
    "\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import vapeplot\n",
    "import arviz as az\n",
    "\n",
    "# modeling\n",
    "import pystan\n",
    "import stan_utility\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed5zssxrF6Eb"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1VZB7OgpF6Ec"
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn')\n",
    "plt.rc('figure', figsize=(11.0, 7.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZtJblGFF6Eg"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lndHvFjsF6Eh"
   },
   "outputs": [],
   "source": [
    "file_path = r'../../data/snow_data_clean.parquet'\n",
    "all_data_path = os.path.join(os.getcwd(), file_path)\n",
    "model_path = r'./stan_model.pkl'\n",
    "result_path = r'../../data/processed/stan_results.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "jiLDY-_dF6Ek",
    "outputId": "667cd564-a92a-4a70-d6ba-2ea4f5a3a5b1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dayofyr</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>base</th>\n",
       "      <th>station</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>ski_yr</th>\n",
       "      <th>state</th>\n",
       "      <th>region</th>\n",
       "      <th>pseudo_ts_delt</th>\n",
       "      <th>pseudo_ski_yr</th>\n",
       "      <th>pseudo_ts</th>\n",
       "      <th>basecol_interpolated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11085</th>\n",
       "      <td>137.0</td>\n",
       "      <td>2016-01-10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Mt. Holiday</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>michigan</td>\n",
       "      <td>Other</td>\n",
       "      <td>324.0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>1692-01-10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11086</th>\n",
       "      <td>138.0</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>-2.320142</td>\n",
       "      <td>Mt. Holiday</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>michigan</td>\n",
       "      <td>Other</td>\n",
       "      <td>324.0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>1692-01-11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11087</th>\n",
       "      <td>139.0</td>\n",
       "      <td>2016-01-12</td>\n",
       "      <td>-2.320142</td>\n",
       "      <td>Mt. Holiday</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>michigan</td>\n",
       "      <td>Other</td>\n",
       "      <td>324.0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>1692-01-12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11088</th>\n",
       "      <td>140.0</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>6.737995</td>\n",
       "      <td>Mt. Holiday</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>michigan</td>\n",
       "      <td>Other</td>\n",
       "      <td>324.0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>1692-01-13</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11089</th>\n",
       "      <td>141.0</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>Mt. Holiday</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>michigan</td>\n",
       "      <td>Other</td>\n",
       "      <td>324.0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>1692-01-14</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dayofyr  timestamp       base      station  snowfall  ski_yr     state  \\\n",
       "11085    137.0 2016-01-10   0.000000  Mt. Holiday       2.0     5.0  michigan   \n",
       "11086    138.0 2016-01-11  -2.320142  Mt. Holiday       3.0     5.0  michigan   \n",
       "11087    139.0 2016-01-12  -2.320142  Mt. Holiday       5.0     5.0  michigan   \n",
       "11088    140.0 2016-01-13   6.737995  Mt. Holiday       0.0     5.0  michigan   \n",
       "11089    141.0 2016-01-14  10.000000  Mt. Holiday       0.0     5.0  michigan   \n",
       "\n",
       "      region  pseudo_ts_delt  pseudo_ski_yr  pseudo_ts  basecol_interpolated  \n",
       "11085  Other           324.0          -31.0 1692-01-10                  True  \n",
       "11086  Other           324.0          -31.0 1692-01-11                  True  \n",
       "11087  Other           324.0          -31.0 1692-01-12                  True  \n",
       "11088  Other           324.0          -31.0 1692-01-13                 False  \n",
       "11089  Other           324.0          -31.0 1692-01-14                 False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parquet opening is broken on colab\n",
    "with open(file_path, 'rb') as parq_file:\n",
    "    long_series_df = pd.read_parquet(parq_file)\n",
    "assert long_series_df.base.isna().sum()==0\n",
    "\n",
    "long_series_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vfg37-k3F6En"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dcKhhazNF6En"
   },
   "outputs": [],
   "source": [
    "def add_month(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    return data.assign(month=lambda x:\n",
    "                       x.pseudo_ts.dt.month)\n",
    "\n",
    "def add_diff(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" use difference in base, not absolute value \"\"\"\n",
    "    return (data\n",
    "            .assign(delta_base=lambda x: x.base.diff(1))\n",
    "            .fillna(0)\n",
    "            .drop(columns=['base'])\n",
    "           )\n",
    "\n",
    "def ohe(data: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    return pd.concat([data.drop(columns=[col]),\n",
    "                      pd.get_dummies(data[col],\n",
    "                                     prefix=col)],\n",
    "                     axis=1)\n",
    "\n",
    "def add_month_x_snowfall(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"adds interaction terms\"\"\"\n",
    "    months = [col for col in data.columns\n",
    "              if 'month_' in col]\n",
    "    combos_df = pd.concat([pd.Series(data.snowfall * data[month],\n",
    "                                     name='snowfall_x_' + month)\n",
    "                           for month in months], axis=1)\n",
    "    return pd.concat([data, combos_df], axis=1)\n",
    "\n",
    "def cleaner(data: pd.DataFrame, includes: list=[None]) -> pd.DataFrame:\n",
    "    \"\"\" Removes interpolated rows and unneeded columns\n",
    "    Params:\n",
    "        data: df to operate on\n",
    "        includes: column names NOT to drop (don't need to specify usually)\n",
    "    ski_yr is needed for test/train split\"\"\"\n",
    "    data = data.query('basecol_interpolated==False')\n",
    "    bad_cols = ['dayofyr', 'station', 'state', 'pseudo_ski_yr',\n",
    "                'timestamp', 'basecol_interpolated', 'pseudo_ts',\n",
    "                'pseudo_ts_delt'\n",
    "               ]\n",
    "    bad_cols = [col for col in bad_cols if col not in includes]\n",
    "    return data.drop(columns=bad_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "AI5Yw01cF6Er",
    "outputId": "4b0a6a64-652d-445f-e967-b03b2bdb74d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "snowfall                                                          239805\n",
       "ski_yr                                                            847944\n",
       "region                 OtherOtherOtherOtherOtherOtherOtherOtherOtherO...\n",
       "delta_base                                                       57308.9\n",
       "month_1                                                            53318\n",
       "month_2                                                            47936\n",
       "month_3                                                            45183\n",
       "month_4                                                            14427\n",
       "month_5                                                             1734\n",
       "month_6                                                              357\n",
       "month_7                                                               94\n",
       "month_8                                                               14\n",
       "month_9                                                                3\n",
       "month_10                                                             328\n",
       "month_11                                                            8716\n",
       "month_12                                                           39818\n",
       "snowfall_x_month_1                                                 54429\n",
       "snowfall_x_month_2                                                 61551\n",
       "snowfall_x_month_3                                                 47591\n",
       "snowfall_x_month_4                                                 13534\n",
       "snowfall_x_month_5                                                   982\n",
       "snowfall_x_month_6                                                    33\n",
       "snowfall_x_month_7                                                     0\n",
       "snowfall_x_month_8                                                     0\n",
       "snowfall_x_month_9                                                     0\n",
       "snowfall_x_month_10                                                  242\n",
       "snowfall_x_month_11                                                 9598\n",
       "snowfall_x_month_12                                                51845\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (long_series_df.pipe(add_month)\n",
    "        .pipe(add_diff)\n",
    "        .pipe(ohe, 'month')\n",
    "        .pipe(add_month_x_snowfall)\n",
    "        .pipe(cleaner)\n",
    ")\n",
    "data.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m1rP0cIF6FR"
   },
   "source": [
    "# Bayesian Model in Stan (MCMC)\n",
    "I want to add priors to the model that snowfall should only result in increases in base depth, and monthly effects should only result in reduction (i.e., monthly effect should measure strength of melting.); changes at odds with this should be considered as noise. A bayesian model allows for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "__OGwbNgF6FR"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snowfall</th>\n",
       "      <th>ski_yr</th>\n",
       "      <th>delta_base</th>\n",
       "      <th>region_Cascades</th>\n",
       "      <th>region_Colorado</th>\n",
       "      <th>region_East</th>\n",
       "      <th>region_New_England</th>\n",
       "      <th>region_Other</th>\n",
       "      <th>region_Rockies_Other</th>\n",
       "      <th>region_Sierras</th>\n",
       "      <th>...</th>\n",
       "      <th>month_3</th>\n",
       "      <th>month_4</th>\n",
       "      <th>month_5</th>\n",
       "      <th>month_6</th>\n",
       "      <th>month_7</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11088</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.058137</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11089</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.262005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11090</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11091</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11092</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       snowfall  ski_yr  delta_base  region_Cascades  region_Colorado  \\\n",
       "11088       0.0     5.0    9.058137                0                0   \n",
       "11089       0.0     5.0    3.262005                0                0   \n",
       "11090       0.0     5.0    0.000000                0                0   \n",
       "11091       0.0     5.0    0.000000                0                0   \n",
       "11092       0.0     5.0    0.000000                0                0   \n",
       "\n",
       "       region_East  region_New_England  region_Other  region_Rockies_Other  \\\n",
       "11088            0                   0             1                     0   \n",
       "11089            0                   0             1                     0   \n",
       "11090            0                   0             1                     0   \n",
       "11091            0                   0             1                     0   \n",
       "11092            0                   0             1                     0   \n",
       "\n",
       "       region_Sierras  ...  month_3  month_4  month_5  month_6  month_7  \\\n",
       "11088               0  ...        0        0        0        0        0   \n",
       "11089               0  ...        0        0        0        0        0   \n",
       "11090               0  ...        0        0        0        0        0   \n",
       "11091               0  ...        0        0        0        0        0   \n",
       "11092               0  ...        0        0        0        0        0   \n",
       "\n",
       "       month_8  month_9  month_10  month_11  month_12  \n",
       "11088        0        0         0         0         0  \n",
       "11089        0        0         0         0         0  \n",
       "11090        0        0         0         0         0  \n",
       "11091        0        0         0         0         0  \n",
       "11092        0        0         0         0         0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stan_df = (long_series_df\n",
    "           .pipe(add_month)\n",
    "           .pipe(add_diff)\n",
    "           .pipe(ohe, 'region')\n",
    "           .pipe(ohe, 'month')\n",
    "           .pipe(cleaner)\n",
    "           )\n",
    "stan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "f_7mt-C1Vi60"
   },
   "outputs": [],
   "source": [
    "# sample data; half million records => slow mcmc\n",
    "#stan_sample_full_df = (stan_df.sample(frac=.16, axis=0, replace=False))\n",
    "\n",
    "def sample_weighted_season(df: pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"samples dataframe but doesn't remove rare months and mildly reduces\n",
    "    amount of semi-rare months\"\"\"\n",
    "    # un-OHE\n",
    "    df['month'] = df[[c for c in data.columns if \"month_\" in c and \"x_m\" not in c]].idxmax(axis=1)\n",
    "    # define months\n",
    "    rare_months = [f'month_{i}' for i in range(5,11)]\n",
    "    semirare_months = ['month_4', 'month_11']\n",
    "    nonrare_months = ['month_12', 'month_1', 'month_2', 'month_3']\n",
    "    # split and sample data\n",
    "    rare_data = df.query('month in @rare_months')\n",
    "    semirare_data = df.query('month in @semirare_months').sample(frac=.3, axis=0)\n",
    "    nonrare_data = df.query('month in @nonrare_months').sample(frac=.09, axis=0)\n",
    "    # recombine\n",
    "    return pd.concat([rare_data, semirare_data, nonrare_data], axis=0).drop(columns=['month'])\n",
    "\n",
    "# split data first so rare months get included in both sets\n",
    "stan_sample_test_df = stan_df.sample(frac=.20, axis=0)\n",
    "stan_sample_train_df = stan_df.drop(index=stan_sample_test_df.index)\n",
    "# reduce data size \n",
    "stan_sample_test_df = stan_df.pipe(sample_weighted_season)\n",
    "stan_sample_train_df = stan_df.pipe(sample_weighted_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RL_pNkXf5Vob"
   },
   "outputs": [],
   "source": [
    "# provide data including shapes and column type locations to stan\n",
    "columns = stan_df.columns\n",
    "region_cols = [c for c in columns if \"region\" in c]\n",
    "month_cols = [col for col in stan_sample_train_df.columns if \"month\" in col]\n",
    "\n",
    "X = stan_sample_train_df.drop(columns=['delta_base'])\n",
    "X_month= X[month_cols]\n",
    "X_snow = X['snowfall']\n",
    "X_region = X[region_cols]\n",
    "y = stan_sample_train_df[['delta_base']]\n",
    "\n",
    "X_test = stan_sample_test_df.drop(columns=['delta_base'])\n",
    "X_month_test = X_test[month_cols]\n",
    "X_snow_test = X_test['snowfall']\n",
    "X_region_test = X_test[region_cols]\n",
    "y_test = stan_sample_test_df[['delta_base']]\n",
    "\n",
    "stan_data = {'N': X.shape[0],\n",
    "             'K_month': X_month.shape[1],\n",
    "             'X_month': X_month.to_numpy(),\n",
    "             'K_reg': X_region.shape[1],\n",
    "             'X_reg': X_region.to_numpy(),\n",
    "             'X_snow': X_snow.to_numpy().reshape(-1,1),\n",
    "             'y': y.to_numpy().reshape(-1),\n",
    "             # test\n",
    "             'N_test': X_test.shape[0],\n",
    "             'X_month_test': X_month_test.to_numpy(),\n",
    "             'X_reg_test': X_region_test.to_numpy(),\n",
    "             'X_snow_test': X_snow_test.to_numpy().reshape(-1,1),\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Rxc7NygqF6FU"
   },
   "outputs": [],
   "source": [
    "stan_model_str = \"\"\"\n",
    "functions {}\n",
    "data {\n",
    "    // input data passed from Python\n",
    "    int<lower=1> N;               // number of data observations\n",
    "    int<lower=1> K_month;         // no of melting predictor\n",
    "    matrix[N, K_month] X_month;   // predictor for melting features\n",
    "    int<lower=1> K_reg;           // no of region features\n",
    "    matrix[N, K_reg] X_reg;       // region predictors\n",
    "    matrix[N, 1] X_snow;          // snowfall predictor\n",
    "    vector[N] y;                  // response vector\n",
    "    \n",
    "    // test variables\n",
    "    int<lower=1> N_test;                  // no of test records\n",
    "    matrix[N_test, K_month] X_month_test; // predictor for melting features\n",
    "    matrix[N_test, K_reg] X_reg_test;     // region predictors\n",
    "    matrix[N_test, 1] X_snow_test;\n",
    "}\n",
    "transformed data {\n",
    "    matrix[N, K_reg] X_reg_snow;\n",
    "    row_vector[N] X_snow_rvect = to_row_vector(X_snow);\n",
    "    matrix[N_test, K_reg] X_reg_snow_test;\n",
    "    row_vector[N_test] X_snow_rvect_test = to_row_vector(X_snow_test);\n",
    "    \n",
    "    for (k in 1:K_reg) {          //  K_regxN * Nx1  T\n",
    "        for (n in 1:N) {\n",
    "            X_reg_snow[n,k] = X_snow_rvect[n] * X_reg[n,k];\n",
    "    }  }\n",
    "    \n",
    "    // same, but for test. Should do this with a function...\n",
    "    for (k in 1:K_reg) {\n",
    "        for (n in 1:N_test) {\n",
    "            X_reg_snow_test[n,k] = X_snow_rvect_test[n] * X_reg_test[n,k];\n",
    "    }  }\n",
    "}\n",
    "parameters {\n",
    "    // intercept was causing divergences and coef interpretation \n",
    "    // makes more sense without intercept: \n",
    "    // I don't expect change in base depth absent melting or snowfall\n",
    "    vector<upper=0>[K_month] beta_mo;           // coefficients for melting\n",
    "    vector<lower=0, upper=1>[K_reg] beta_reg_snow;       // coef for region x snow interaction\n",
    "    real<lower=0> sigma;                        // must be +ve\n",
    "    real<lower=0> sig_mos;                      // must be +ve\n",
    "}\n",
    "transformed parameters {\n",
    "    vector[N] mu;                       // y_hat\n",
    "    mu = X_month*beta_mo + X_reg_snow*beta_reg_snow;\n",
    "}\n",
    "model {\n",
    "    sigma ~ cauchy(0, 10);              // half Cauchy\n",
    "    sig_mos ~ cauchy(0, 20);\n",
    "    for (n in 1:K_month) {\n",
    "        beta_mo[n] ~ normal(0, sig_mos) T[,0]; // sample from normal, only -ve\n",
    "    }\n",
    "    // prior on snow columns is beta over [0,1]\n",
    "    beta_reg_snow ~ beta(2.2, 3);         // reparameterize so this and snow are from beta dist\n",
    "    y ~ normal(mu, sigma);\n",
    "}\n",
    "generated quantities{\n",
    "    //vector[N_test] y_test;\n",
    "    //for(n in 1:N_test) {\n",
    "    //    y_test[n] = normal_rng(X_month_test[n]*beta_mo + \n",
    "    //                           X_reg_snow_test[n]*beta_reg_snow, sigma);\n",
    "  //}\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ffQAVs5kt23C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL stan_model_4050408f807d7ea68e97902ffb2d7608 NOW.\n"
     ]
    }
   ],
   "source": [
    "sm = pystan.StanModel(model_code=stan_model_str, model_name='stan_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JT3bX-l5ocXx"
   },
   "outputs": [],
   "source": [
    "# avoid recompile if possible\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(sm, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wD_ZZ4pquEND"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n"
     ]
    }
   ],
   "source": [
    "fit = sm.sampling(data=stan_data, iter=2_000, chains=4, n_jobs=-1,\n",
    "                  sample_file=\"../../data/processed/stan_samples\",\n",
    "                  control={'adapt_delta': 0.85, # p accepting posterior draw\n",
    "                           'stepsize': 1,  # just starting stepsize\n",
    "                          }, \n",
    "                  seed=42, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "YJXVG5wGhUeI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-5b6186a353e9>:4: UserWarning: Pickling fit objects is an experimental feature!\n",
      "The relevant StanModel instance must be pickled along with this fit object.\n",
      "When unpickling the StanModel must be unpickled first.\n",
      "  pickle.dump(fit, f)\n"
     ]
    }
   ],
   "source": [
    "# for overnight run\n",
    "try:\n",
    "    with open(result_path, 'wb') as f:\n",
    "        pickle.dump(fit, f)\n",
    "# reload saved objects if not reruning sampler\n",
    "except NameError:\n",
    "    with open(model_path, 'rb') as f:\n",
    "        sm = pickle.load(f)\n",
    "    with open(result_path, 'rb') as f:\n",
    "        fit = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWB3_b-Fx7Dp"
   },
   "source": [
    "## MCMC Diagnostics\n",
    "We will want to check:\n",
    "1. Model actually runs.\n",
    "1. Good Mixing of Chains: (fix with stronger prior, reparameterization)\n",
    "    1. $\\hat{R}$ is 1.1 or under for all parameters.\n",
    "    1. When n_eff / n_transitions < 0.001 the estimators that we use are often biased and can significantly overestimate the true effective sample size.\n",
    "1. Check tree depth:\n",
    "if threshold saturated, increase tree depth _control={max_treedepth: 15}_\n",
    "1. \n",
    "\n",
    "_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JSPHbBtmns3M"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ae5562fa4833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstan_utility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_all_diagnostics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/stan_utility/utils.py\u001b[0m in \u001b[0;36mcheck_all_diagnostics\u001b[0;34m(fit, max_treedepth, quiet)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mcheck_n_eff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mcheck_rhat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mcheck_div\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/stan_utility/utils.py\u001b[0m in \u001b[0;36mcheck_n_eff\u001b[0;34m(fit, quiet)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_n_eff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;34m\"\"\"Checks the effective sample size per iteration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mfit_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mn_effs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfit_summary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_summary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary_rownames'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mstanfit4stan_model_4050408f807d7ea68e97902ffb2d7608_1645982641195856552.pyx\u001b[0m in \u001b[0;36mstanfit4stan_model_4050408f807d7ea68e97902ffb2d7608_1645982641195856552.StanFit4Model.summary\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pystan/misc.py\u001b[0m in \u001b[0;36m_summary\u001b[0;34m(fit, pars, probs, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.975\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0mss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_summary_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m     \u001b[0;31m# TODO: include sem, ess and rhat: ss['ess'], ss['rhat']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'msd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sem'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'msd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quan'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ess'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rhat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pystan/misc.py\u001b[0m in \u001b[0;36m_summary_sim\u001b[0;34m(sim, pars, probs)\u001b[0m\n\u001b[1;32m    360\u001b[0m                         \u001b[0mstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                         chains=tuple(\"chain:{}\".format(cid) for cid in cids))\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mess_and_rhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpystan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mess_and_splitrhat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtidx_colm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0mess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mess_and_rhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     return dict(msd=msd, c_msd=c_msd, c_msd_names=c_msd_names, quan=quan,\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pystan/misc.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    360\u001b[0m                         \u001b[0mstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                         chains=tuple(\"chain:{}\".format(cid) for cid in cids))\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mess_and_rhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpystan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mess_and_splitrhat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtidx_colm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0mess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mess_and_rhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     return dict(msd=msd, c_msd=c_msd, c_msd_names=c_msd_names, quan=quan,\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pystan/chains.py\u001b[0m in \u001b[0;36mess_and_splitrhat\u001b[0;34m(sim, n)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# FIXME: does not yet save time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitrhat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pystan/chains.py\u001b[0m in \u001b[0;36mess\u001b[0;34m(sim, n)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_chains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_sample_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZeroDivisionError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stan_utility.check_all_diagnostics(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtAQQbXgoJM4"
   },
   "outputs": [],
   "source": [
    "# fix brackets in col nmaes\n",
    "fit_df = (fit.to_dataframe()\n",
    "          .rename(columns=lambda x: x.replace(\"[\", \"_\"))\n",
    "          .rename(columns=lambda x: x.replace(\"]\", \"\")))\n",
    "\n",
    "fit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiBiPS4n-Boz"
   },
   "source": [
    "## Visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-i-s8UKnjA8"
   },
   "outputs": [],
   "source": [
    "fit_az = az.from_pystan(posterior=fit,\n",
    "                        dims={'beta_reg_snow': ['Coefficients_for_Snow_by_Region'],\n",
    "                              'beta_mo': ['Melting_Coefficients_by_Month']},\n",
    "                        coords={'Coefficients_for_Snow_by_Region': X_region.columns.values.tolist(),\n",
    "                                'Melting_Coefficients_by_Month': [calendar.month_name[i+1] for i in range(12)]}\n",
    "                        )\n",
    "rc = {'plot.max_subplots': None}\n",
    "az.rcParams.update(rc)\n",
    "sns.set_style('whitegrid')\n",
    "az.plot_trace(fit_az)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnInGaRAaRfY"
   },
   "outputs": [],
   "source": [
    "# get region names without \"region_\"\n",
    "region_names = [reg[7:] for reg in X_region.columns.values.tolist()]\n",
    "\n",
    "region_betas_df = fit_df.filter(regex=\"reg\", axis=1)\n",
    "reg_cols = region_betas_df.columns\n",
    "region_betas_df = (region_betas_df\n",
    "                   .rename(columns={col: reg_name for col, reg_name \n",
    "                                    in zip(reg_cols, region_names)})\n",
    "                   .melt(var_name=\"region\"))\n",
    "region_betas_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uE97qf4OZxiU"
   },
   "outputs": [],
   "source": [
    "plt.style.use('bmh')\n",
    "fig = sns.kdeplot(x=region_betas_df.value, hue=region_betas_df.region, fill=True, cut=0, bw_adjust=.3)\n",
    "plt.suptitle(\"Estimated Base Increase per Unit of Snowfall\", fontsize=20)\n",
    "plt.xlabel(\"Effect of Unit of Powder\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2D_363pltuR"
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "jazzcup = sns.blend_palette(vapeplot.palette(\"jazzcup\"), n_colors=region_betas_df.region.unique().size)\n",
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "sort_order = region_betas_df.groupby(['region']).mean().sort_values(by='value', ascending=True).index\n",
    "\n",
    "sns.violinplot(x='region', y='value', data=region_betas_df,\n",
    "            order=sort_order, palette=jazzcup)\n",
    "\n",
    "plt.title(\"Estimated Base Increase per Unit of Snowfall: Bayesian Model\", fontsize=20)\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Fraction of Full Unit of Powder');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twx31uBSsgyN"
   },
   "outputs": [],
   "source": [
    "month_betas_df = fit_df.filter(like='beta_mo').melt(var_name=\"month\")\n",
    "month_betas_df = month_betas_df[month_betas_df.value > month_betas_df.value.quantile(.02)]\n",
    "month_map = {f\"beta_mo_{i}\": calendar.month_abbr[i] for i in range(1, 13)}\n",
    "#month_betas_df['month'] = pd.to_datetime(month_betas_df['month'].replace(month_map), format=\"%B\").dt.month.astype('category')\n",
    "month_betas_df['month'] = month_betas_df['month'].replace(month_map).astype('str')\n",
    "month_betas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxXKPQVlt_gB"
   },
   "outputs": [],
   "source": [
    "def plot_snow_betas(df, start_mo):\n",
    "    fig, ax = plt.subplots()\n",
    "    month_ordered = [mo for mo in calendar.month_abbr[1:] if mo in df.month.unique()]\n",
    "    start_mo_ix = month_ordered.index(start_mo)\n",
    "    month_ordered = month_ordered[start_mo_ix:] + month_ordered[:start_mo_ix]\n",
    "    sns.boxplot(data=df, y='value', x='month', order=month_ordered,\n",
    "                ax=ax, )\n",
    "    ax.set_ylabel('Inches Melted per Day')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_title('Estimated Snow Melted per Day by Month');\n",
    "plot_snow_betas(month_betas_df, \"Jan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPhSA24RJfJD"
   },
   "source": [
    "These estimates are mostly expected, but there seems to be low melting amounts during summer...this can be explained when we realize that most of the values for May-November were interpolated. The averages aren't weighted by ski acreage, so the large number of small ski stations on the east coast & midwest with short seasons are disproportionately affecting these numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blPU-QZ4HHce"
   },
   "outputs": [],
   "source": [
    "interpo_ratios=(long_series_df\n",
    "    .assign(month=lambda x: x.pseudo_ts.dt.month)\n",
    "    .groupby('month')\n",
    "    .apply(lambda x: x.basecol_interpolated.sum()/x.shape[0])\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .rename(columns={0:'ratio'})\n",
    ")\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(data=interpo_ratios, x='month', y='ratio', ax=ax)\n",
    "plt.title('Fraction of Base observations that were interpolated', fontsize=15)\n",
    "[plt.text((i-.17), value+.01, str(value)) for i, value in enumerate(interpo_ratios.ratio.round(2).to_numpy())]\n",
    "months_xticks(ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhsp01-EJze7"
   },
   "outputs": [],
   "source": [
    "plot_snow_betas(month_betas_df[~month_betas_df.month.isin(['Jun', 'Jul', 'Aug', 'Sep', 'Oct'])], \"Nov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJgH7yRoKqRv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "snow_nonts_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
